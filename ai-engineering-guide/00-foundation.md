
# ðŸ”¬ Foundations: State of the Art in AI

A good AI engineer or researcher must understand the building blocks. Here's a structured list of core and cutting-edge topics:

## âœ… Core Concepts

- **Perceptron, ReLU**  
  How a network shares data and processes it.

- **Gradient Descent, Adam Optimizer, Backpropagation**  
  The equations of learning.

- **Loss Functions**  
  MSE, Cross-Entropy, KL Divergence, etc.

- **Feedforward Neural Networks, CNNs, RNNs, etc**  
  The most classical concepts in deep learning.

## âœ… Advanced Topics

- **Variational Autoencoders (VAE)**  
  A way to compress information in a network and analyze how the AI interprets data. Foundation for latent space understanding and Nomic AI (the beginning of embeddings).

- **Generative Adversarial Networks (GANs)**  
  The real beginning of AI in art. Still used in modern architectures like diffusion models and various loss strategies.

- **Contrastive Languageâ€“Image Pre-training (CLIP)**  
  Foundation of multimodal learning.

- **Large Language Models (LLMs)**  
  GPT, LLaMA, Qwen, etc.

- **Mixture of Agents**  
  Example: DeepSeek. Combining multiple models to act as one and select the best outputs.

- **Multimodal Systems**  
  Gato, Gemini, Flamingo, etc.

### Reinforcement Learning (RL)

- **Deep Q-Learning**  
- **Policy Gradients**  
- **PPO, A3C, SAC**, etc.

### Other Concepts

- **Latent Spaces**  
  Interpretation & manipulation.

- **Prompt Engineering & Evaluation**

- **Verification Models**  
  Output confidence & self-verification.

- **XAI (Explainable AI)**  
  Making models understandable and proving they are unbiased.

- **Graph Neural Networks (GNNs)**  
  An innovative approach used by Google. Less feedforward, more interconnected.

- **Liquid Neural Networks**  
  Currently more theoretical. Hard to understand and train, with minor gains. Considered inefficient by many, but intriguingly beautiful.

